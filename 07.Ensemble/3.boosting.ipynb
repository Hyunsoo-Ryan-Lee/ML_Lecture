{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 부스팅(Boosting)\n",
    "\n",
    "- 여러 개의 모델이 순차적으로 학습을 진행\n",
    "- 이전 모델이 예측이 틀린 데이터에 대해 올바르게 예측할 수 있도록 다음 모델에게 가중치를 부여하면서 학습과 예측을 진행\n",
    "- 오류가 큰 데이터에 더 많은 가중치를 부여하여 모델을 강화\n",
    "- 계속하여 모델에게 가중치를 부스팅하며 학습을 진행하기에 부스팅 방식이라고 불림\n",
    "- 순차적으로 학습하기에 배깅 방식과는 달리 훈련을 동시에 진행할 수 없으므로 훈련시간이 오래걸림\n",
    "- 대표적인 부스팅 알고리즘 : Gradient Boosting, AdaBoost, XGBoost\n",
    "\n",
    "![](https://velog.velcdn.com/images/newnew_daddy/post/91a8df59-7a6a-4f7a-a324-c70d972f3803/image.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### 배깅 vs 부스팅\n",
    "- 배깅 : 여러 학습 모델을 병렬적으로 학습하여 각각의 예측 결과를 평균내거나 투표를 통해 결합\n",
    "- 부스팅 : 학습 모델을 순차적으로 학습시키며, 각 모델이 이전 모델의 오류를 보완하도록 가중치를 조정하여 결합\n",
    "\n",
    "![](https://velog.velcdn.com/images/newnew_daddy/post/a4b64e88-06a1-4487-a30f-ab8e7a71bce0/image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. AdaBoost\n",
    "- Adaptive + Boosting의 줄임말\n",
    "- 약한 분류기(weak classifier)들이 상호보완 하도록 순차적(sequential)으로 학습하고, 이들을 조합하여 최종적으로 강한 분류기(strong classifier)의 성능을 향상\n",
    "- 작동 방법\n",
    "  1. 성능이 좋지 않은 약한 분류기(weak classifier)들은 한 번에 하나씩 순차적으로 학습을 진행\n",
    "  2. 먼저 학습된 분류기가 제대로 분류한 결과 정보와 잘못 분류한 결과 정보를 다음 분류기에 전달\n",
    "  3. 다음 분류기는 이전 분류기로부터 받은 정보를 활용하여 잘 분류해내지 못한 데이터들의 가중치(weight)를 높인다.(가중치를 adaptive하게 바꿈)\n",
    "  4. 최종 분류기(strong classifier)는 이전에 학습한 약한 분류기들에 각각 가중치를 적용하고 조합하여 학습을 진행\n",
    "\n",
    "![](https://velog.velcdn.com/images/newnew_daddy/post/2dc5dc94-652a-4b0b-bde6-76d1e33f07aa/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gradient Boosting\n",
    "- Gradient Descent + Boosting의 줄임말\n",
    "- 약한 학습기(weak learner)들이 순차적으로 학습되며, 각 단계에서 이전 학습기의 오류를 줄이는 방향으로 모델을 최적화\n",
    "- 작동 방법\n",
    "  1. 첫 번째 약한 학습기(weak learner)가 초기 데이터를 학습하여 예측 모델을 생성\n",
    "  2. 이 모델이 만든 예측과 실제 값의 차이(잔차, residual)를 계산\n",
    "  3. 다음 학습기에서는 이 잔차를 학습하여 예측 모델을 개선\n",
    "  4. 이 과정에서 손실 함수를 최소화하기 위해 경사 하강법(gradient descent)을 사용하여 최적화\n",
    "  5. 최종 모델(strong learner)은 모든 약한 학습기들의 예측을 합산하여 예측을 수행, 각 학습기의 기여도는 학습 과정에서 조정됨\n",
    "\n",
    "![](https://velog.velcdn.com/images/newnew_daddy/post/cdb51e5b-22f3-45f1-9ddf-bb7cb05c4080/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. XGBoost\n",
    "- eXtreme Gradient Boosting의 줄임말\n",
    "- Gradient Boosting을 기반으로 한 고성능 앙상블 기법으로, 경량화된 구현과 성능 향상을 위한 다양한 기술을 포함\n",
    "- 작동 방법\n",
    "  1. 기본적으로 Gradient Boosting과 유사하게 작동하나, 추가적으로 정규화(term regularization)와 같은 기법을 포함하여 과적합을 방지\n",
    "  2. 병렬 처리를 지원하여 학습 속도를 크게 향상\n",
    "  3. 각 학습 단계에서 최적의 분할을 찾기 위해 지니 계수(Gini coefficient)나 엔트로피(entropy)를 활용\n",
    "  4. 모델 평가를 위해 교차 검증(cross-validation)을 활용하여 최적의 모델을 찾고, 조기 종료(early stopping) 기능으로 불필요한 학습을 방지\n",
    "  5. 최종 모델은 여러 개의 약한 학습기들의 조합으로 구성되며, 각 학습기의 중요도를 반영하여 최종 예측을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
